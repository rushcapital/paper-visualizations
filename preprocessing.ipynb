{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import shutil\n",
    "import errno\n",
    "\n",
    "def copyAnything(src, dst):\n",
    "    try:\n",
    "        shutil.copytree(src, dst)\n",
    "    except OSError as exc: # python >2.5\n",
    "        if exc.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "            shutil.copy(src, dst)\n",
    "        else: raise\n",
    "\n",
    "def findWells(wellListFile, wellFolders):\n",
    "    # get the apiNo's we found in the drillingInfo query.\n",
    "\n",
    "    bakkenWells = pd.read_csv(wellListFile)\n",
    "    apiColumn = bakkenWells.columns[0]\n",
    "    apiNums = set(map(lambda x: int(x/1e4), bakkenWells[apiColumn]))\n",
    "\n",
    "    # get the list of wells in our north dakota directory (without checking for LAS present).\n",
    "    folderList = glob(f'{wellFolders}/*', recursive=True)\n",
    "    refinedList = set([int(x.split('\\\\')[1]) for x in folderList])\n",
    "    \n",
    "    # # check if we have folders for any of the wells found in the drillinginfo set.\n",
    "    intersection = list(refinedList.intersection(apiNums))\n",
    "\n",
    "    # given the new wellset, derive a new directory with only folders from this subset\n",
    "    if not os.path.exists('wellSubset'):\n",
    "        os.makedirs('wellSubset')\n",
    "\n",
    "    newPath = f'wellSubset\\\\'\n",
    "    for folder in intersection:\n",
    "        oldPath = f'scrapedWells\\\\{folder}\\\\'\n",
    "        _new = f'{newPath}{folder}'\n",
    "        try:\n",
    "            copyAnything(oldPath, _new)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "      \n",
    "def checkForLAS(wellFolders):\n",
    "    folderList = glob(f'{wellFolders}/*', recursive=True)\n",
    "    for folder in folderList:\n",
    "        files = glob(f'{folder}/*')\n",
    "        if any('.las' in s for s in files):\n",
    "            pass\n",
    "        else:\n",
    "            print(f'no LAS files found. removing {folder}')\n",
    "            try:\n",
    "                shutil.rmtree(folder)\n",
    "            except OSError as err:\n",
    "                print(\"Error: %s : %s\" % (folder, err.strerror))\n",
    "    \n",
    "def parseLASv1(LAS):\n",
    "    with open(LAS,'r') as f:\n",
    "        \"\"\"\n",
    "        for each line, if its not a new line char, split the spaces out.\n",
    "        once only valid lines remain, reverse the list of lists, so that we start from the bottom first.\n",
    "        by doing so, we immediately know the shape of our rows, and can traverse through the list of list,\n",
    "        until we encounter a different sized list. then we can just find the first list with +1 shape as our \n",
    "        rows and thats the column headers.\n",
    "        \"\"\"\n",
    "       \n",
    "        lines = [line.strip().split() for line in f.read().splitlines() if line]\n",
    "\n",
    "        delimiter = None\n",
    "        lenLine = None\n",
    "\n",
    "        for idx,line in enumerate(lines):\n",
    "            if '~A' in line[0]:\n",
    "                delimiter = idx\n",
    "                lenLine = len(line)\n",
    "                break\n",
    "\n",
    "        # find column names and assign to dataframe\n",
    "\n",
    "        if lenLine == 1:\n",
    "            columnHeaders = lines[delimiter-2][1:]\n",
    "            dataframe = pd.DataFrame(lines[delimiter+1:],columns=columnHeaders)\n",
    "\n",
    "        if lenLine > 1:\n",
    "            columnHeaders = lines[delimiter][1:]\n",
    "            dataframe = pd.DataFrame(lines[delimiter+1:], columns=columnHeaders)\n",
    "            \n",
    "        # generate new file, and write back to it \n",
    "        dataframe.to_csv(LAS.split('.')[0]+'.csv')\n",
    "\n",
    "def parseLASv2(LAS):\n",
    "    with open(LAS,'r') as f:\n",
    "        # reverse the arrays, find the first deviation away from the current structure\n",
    "        # add an ADDITIONAL filter for empty lists because apparently one isn't enough.\n",
    "        lines = list(filter(None,[line.strip().split() for line in f.read().splitlines() if line]))\n",
    "        \n",
    "        for idx, line in enumerate(lines):\n",
    "            if '~A' in line[0]:\n",
    "                stop = idx\n",
    "                break\n",
    "        \n",
    "        # the column headers are usually within 3 rows of the delimiter\n",
    "        for i in range(0,10):\n",
    "            row = lines[stop-i]\n",
    "            for j in range(len(row)):\n",
    "                if any(string in row[j] for string in ['DEP','Dep','dep']):\n",
    "                    columnHeaders = row[j:]\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "                \n",
    "        dataframe = pd.DataFrame(lines[stop+1:], columns=columnHeaders)\n",
    "        \n",
    "        dataframe.to_csv(LAS.split('.')[0]+'.csv')              \n",
    "\n",
    "_folder = 3310502065\n",
    "\n",
    "fileList =  glob(f'wellSubset\\\\{_folder}\\\\*.las')\n",
    "for file in fileList:\n",
    "    \n",
    "    try:\n",
    "        parseLASv2(file)\n",
    "    except Exception as err:\n",
    "        print(file.split('\\\\')[-1])            \n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beginParsing(parentDirectory):\n",
    "    folderList = glob(f'{parentDirectory}/*', recursive=True)\n",
    "    for folder in folderList:\n",
    "        files = glob(f'{folder}/*.las')\n",
    "        for file in files:\n",
    "            try:\n",
    "                parseLASv2(file)\n",
    "            except Exception as err:\n",
    "                with open('revisit.txt', 'a') as f:\n",
    "                    f.write(file)\n",
    "    \n",
    "beginParsing(parentDirectory='wellSubset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStragglers(file):\n",
    "    with open(file,'r') as f:\n",
    "        data = f.readlines()[0].split('well')[1:]\n",
    "        _d = ['well'+d for d in data]\n",
    "    return _d\n",
    "\n",
    "stragglers = readStragglers('revisit.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def parseLASv3(file):\n",
    "    with open(file, 'r') as f:\n",
    "        lines = list(filter(None,[line.strip().split() for line in f.read().splitlines() if line]))\n",
    "    \n",
    "    for line in lines:\n",
    "        print(line)\n",
    "    for idx, line in enumerate(lines):\n",
    "        if '~A' in line[0]:\n",
    "            stop = idx\n",
    "            break\n",
    "\n",
    "    # the column headers are usually within 3 rows of the delimiter\n",
    "    for i in range(0,10):\n",
    "        row = lines[stop-i]\n",
    "        for j in range(len(row)):\n",
    "            if any(string in row[j] for string in ['DEP','Dep','dep']):\n",
    "                columnHeaders = row[j:]\n",
    "                print(columnHeaders)\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "            \n",
    "    dataframe = pd.DataFrame(lines[stop+1:], columns=columnHeaders)\n",
    "    \n",
    "    # dataframe.to_csv(file.split('.')[0]+'.csv')              \n",
    "\n",
    "\n",
    "parseLASv3(stragglers[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# get updated well list.\n",
    "\n",
    "files = glob('wellSubset/*')\n",
    "wellList = [well.split('\\\\')[1]+'0000' for well in files]\n",
    "f = open('updatedWellList.txt', 'w')\n",
    "for well in wellList:\n",
    "    f.write(well+ '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# read in maxDepth.txt and look at the data, try to see what logs are most common in lateral leg.\n",
    "\n",
    "def findMostCommonFeature(file):\n",
    "    with open(file, 'r') as f:\n",
    "        d = json.loads(f.read())\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    columnDict = {'Depth':0,\n",
    "                'GR':0\n",
    "                }\n",
    "\n",
    "    for k,v in d.items():\n",
    "        # only look at 1000 or so instances. \n",
    "        if i < min(1000, len(d.keys())):\n",
    "            with open('datasets\\\\'+k, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                try:\n",
    "                    columns = lines[0].strip().split(',')[1:]\n",
    "                    columnDict['Depth'] += 1\n",
    "                    for _c in columns[1:]:\n",
    "                        if _c not in columnDict and _c not in ['#Depth', 'DEPTH', 'DEPT']:\n",
    "                            columnDict[_c] = 1\n",
    "                        else:\n",
    "                            columnDict[_c] += 1 \n",
    "                except Exception as err:\n",
    "                    pass\n",
    "        # df = pd.read_csv(k, index_col=0)\n",
    "        i += 1\n",
    "\n",
    "    sortedColumnDict = dict(sorted(columnDict.items(), \n",
    "                                    key=lambda item: item[1],\n",
    "                                    reverse=True))\n",
    "\n",
    "    print(sortedColumnDict)\n",
    "\n",
    "findMostCommonFeature('housekeeping/maxDepths.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "gasList = ['C1', 'C2', 'C3', 'C4']\n",
    "gammaRay = ['Gamma', 'GR','GAMMA']\n",
    "# read in maxDepth.txt and look at the data. \n",
    "# find gas logs, and keep them only. these were the most common features.\n",
    "with open('housekeeping/maxDepths.txt', 'r') as f:\n",
    "    d = json.loads(f.read())\n",
    "    \n",
    "    \n",
    "newDict = {}\n",
    "for k,v in d.items():\n",
    "    k = 'datasets\\\\'+k\n",
    "    \n",
    "    # read in csv, i was going to use csv.DictReader but i dont want first column as unnamed.\n",
    "    df = pd.read_csv(k, index_col=0)\n",
    "    \n",
    "    if all(substring in df.columns for substring in gasList):\n",
    "        grMatch = list(set(df.columns).intersection(set(gammaRay)))\n",
    "        if grMatch:\n",
    "            # restructure depth to be referencable regardless of file.\n",
    "            df = df.rename(columns={ df.columns[0]: \"DEPTH\", grMatch[0]:\"GR\"})\n",
    "            \n",
    "            # generate new well list of pkls\n",
    "            pklPath = k.split('.')[0]+'.pkl'\n",
    "            df.to_pickle(pklPath)\n",
    "            newDict[pklPath] = v\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('housekeeping/maxDepths.txt', 'r') as f:\n",
    "    d = json.loads(f.read())\n",
    "\n",
    "# horizontals = []\n",
    "# for k,v in d.items():\n",
    "#     if v > 14000:\n",
    "#         horizontals.append(k)\n",
    "\n",
    "# with open('housekeeping/horizontalWells.pkl', 'wb') as fp:\n",
    "#     pickle.dump(horizontals, fp)\n",
    "\n",
    "verticals = []\n",
    "for k,v in d.items():\n",
    "    if v < 12000:\n",
    "        print(v)\n",
    "        verticals.append(k)\n",
    "print(len(verticals))\n",
    "with open('../housekeeping/verticalWells.pkl', 'wb') as fp:\n",
    "    pickle.dump(verticals, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('housekeeping/verticalWells.pkl', 'rb') as fp:\n",
    "    d = pickle.load(fp)\n",
    "\n",
    "i = 0\n",
    "\n",
    "columnDict = {'Depth':0,\n",
    "            'GR':0\n",
    "            }\n",
    "\n",
    "for k in d:\n",
    "    # only look at 1000 or so instances. \n",
    "    print(k)\n",
    "    if i < min(1000, len(d)):\n",
    "        with open('datasets\\\\'+k, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            try:\n",
    "                columns = lines[0].strip().split(',')[1:]\n",
    "                columnDict['Depth'] += 1\n",
    "                for _c in columns[1:]:\n",
    "                    if _c not in columnDict and _c not in ['#Depth', 'DEPTH', 'DEPT']:\n",
    "                        columnDict[_c] = 1\n",
    "                    else:\n",
    "                        columnDict[_c] += 1 \n",
    "            except Exception as err:\n",
    "                pass\n",
    "    # df = pd.read_csv(k, index_col=0)\n",
    "    i += 1\n",
    "\n",
    "sortedColumnDict = dict(sorted(columnDict.items(), \n",
    "                                key=lambda item: item[1],\n",
    "                                reverse=True))\n",
    "\n",
    "print(sortedColumnDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'./datasets/horizontalLogs/'+'_'.join(itemlist[0].split('\\\\')[1:3]).split('.')[0] + '.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "bakkenWells = pd.read_csv('middle bakken wells.csv')\n",
    "bakkenWells = bakkenWells.rename(columns={\n",
    "    'Bottom Hole Latitude (WGS84)': 'bh latitude',\n",
    "    'Bottom Hole Longitude (WGS84)': 'bh longitude'\n",
    "})\n",
    "\n",
    "_bakken = bakkenWells[bakkenWells['Producing Reservoir'] == 'MIDDLE BAKKEN (BAKKEN POOL)']\n",
    "x, y, z = _bakken['bh latitude'], _bakken['bh longitude'], _bakken['True Vertical Depth']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from matplotlib import interactive\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "def reject_outliers(data, m=2.):\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d / (mdev if mdev else 1.)\n",
    "    return data[s < m]\n",
    "\n",
    "# read in data and relabel for ease of access.\n",
    "bakkenWells = pd.read_csv('middle bakken wells.csv')\n",
    "bakkenWells = bakkenWells.rename(columns={\n",
    "    'Bottom Hole Latitude (WGS84)': 'bh latitude',\n",
    "    'Bottom Hole Longitude (WGS84)': 'bh longitude',\n",
    "    'True Vertical Depth': 'TVD'\n",
    "})\n",
    "\n",
    "_bakken = bakkenWells[bakkenWells['Producing Reservoir'] == 'MIDDLE BAKKEN (BAKKEN POOL)']\n",
    "\n",
    "# create subset of dataset to derive a surface equation for z-dimension. we'll use a dnn and tensorflow\n",
    "dataset = _bakken[['bh latitude', 'bh longitude', 'TVD']]\n",
    "# drop na rows if any present\n",
    "dataset.dropna()\n",
    "\n",
    "# since 2 axes are longitude and latitude, the only\n",
    "# variance should come STRICTLY from z-axis aka TVD. we want to be pretty strict with how many \n",
    "# std. devs we want to consider for outliers; the underground structure is allowed to be steep,\n",
    "\n",
    "# if you plot a histogram of the data, the distribution is strongly negatively skewed, so using \n",
    "# z-score not the best, unless you kind of know what you're trying to remove. i KNOW i'm removing\n",
    "# a decent chunk of the left tail because too shallow could be anomalous, and a small slice of right\n",
    "# tail where TVD > 16000. in reality, these could represent faults, but it could just mess up the \n",
    "# symmetry of the surface.\n",
    "\n",
    "# plt.hist(dataset['TVD'], bins=1000)\n",
    "\n",
    "dataset = dataset[(np.abs(stats.zscore(dataset['TVD'])) < 4.5)]\n",
    "\n",
    "# plot data to visually determine if there are still outliers. \n",
    "\n",
    "X, Y, Z = dataset['bh latitude'], dataset['bh longitude'], dataset['TVD']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,12),\n",
    "                       subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "ax.scatter(X,Y,Z, c=Z, s=10, cmap='viridis')\n",
    "ax.set_xlabel('Bottom Hole Latitude (WGS84)')\n",
    "ax.set_ylabel('Bottom Hole Longitude (WGS84)')\n",
    "ax.set_zlabel('Middle Bakken Producing Depth (TVD)')\n",
    "ax.invert_zaxis()\n",
    "\n",
    "# looks better.\n",
    "plt.show()\n",
    "\n",
    "# split dataset into training and testing\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('TVD')\n",
    "test_labels = test_features.pop('TVD')\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(train_features))\n",
    "\n",
    "first = np.array(train_features[:1])\n",
    "\n",
    "# with np.printoptions(precision=2, suppress=True):\n",
    "#   print('First example:', first)\n",
    "#   print()\n",
    "#   print('Normalized:', normalizer(first).numpy())\n",
    "\n",
    "def build_and_compile_model(norm):\n",
    "  model = tf.keras.Sequential([\n",
    "      norm,\n",
    "      tf.keras.layers.Dense(8, activation='relu'),\n",
    "      tf.keras.layers.Dense(8, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_absolute_percentage_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    #   plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [TVD]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "dnn_model = build_and_compile_model(normalizer)\n",
    "# dnn_model.summary()\n",
    "\n",
    "history = dnn_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)\n",
    "\n",
    "plot_loss(history)\n",
    "\n",
    "# X, Y = np.meshgrid(X,Y)\n",
    "\n",
    "# # Plot the surface.\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "#                        linewidth=0, antialiased=False)\n",
    "\n",
    "# # Customize the z axis.\n",
    "# ax.set_zlim(-1.01, 1.01)\n",
    "# ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "# # A StrMethodFormatter is used automatically\n",
    "# ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "\n",
    "# # Add a color bar which maps values to colors.\n",
    "# fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats  \n",
    "\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "# read in data and relabel for ease of access.\n",
    "bakkenWells = pd.read_csv('middle bakken wells.csv')\n",
    "bakkenWells = bakkenWells.rename(columns={\n",
    "    'Bottom Hole Latitude (WGS84)': 'bh latitude',\n",
    "    'Bottom Hole Longitude (WGS84)': 'bh longitude',\n",
    "    'True Vertical Depth': 'TVD'\n",
    "})\n",
    "\n",
    "_bakken = bakkenWells[bakkenWells['Producing Reservoir'] == 'MIDDLE BAKKEN (BAKKEN POOL)']\n",
    "\n",
    "# create subset of dataset to derive a surface equation for z-dimension. we'll use a dnn and tensorflow\n",
    "dataset = _bakken[['bh latitude', 'bh longitude', 'TVD']]\n",
    "# drop na rows if any present\n",
    "dataset.dropna()\n",
    "\n",
    "# since 2 axes are longitude and latitude, the only\n",
    "# variance should come STRICTLY from z-axis aka TVD. we want to be pretty strict with how many \n",
    "# std. devs we want to consider for outliers; the underground structure is allowed to be steep,\n",
    "\n",
    "# if you plot a histogram of the data, the distribution is strongly negatively skewed, so using \n",
    "# z-score not the best, unless you kind of know what you're trying to remove. i KNOW i'm removing\n",
    "# a decent chunk of the left tail because too shallow could be anomalous, and a small slice of right\n",
    "# tail where TVD > 16000. in reality, these could represent faults, but it could just mess up the \n",
    "# symmetry of the surface.\n",
    "\n",
    "# plt.hist(dataset['TVD'], bins=1000)\n",
    "\n",
    "dataset = dataset[(np.abs(stats.zscore(dataset['TVD'])) < 4.5)]\n",
    "\n",
    "# plot data to visually determine if there are still outliers. \n",
    "\n",
    "X, Y, Z = dataset['bh latitude'], dataset['bh longitude'], dataset['TVD']\n",
    "\n",
    "# to Add a color bar which maps values to colors.\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf=ax.plot_trisurf(Y, X, Z, cmap=plt.cm.viridis,\n",
    "                     edgecolor='none', linewidth=0, antialiased=False)\n",
    "cbar = fig.colorbar( surf, shrink=0.5, aspect=5, )\n",
    "cbar.set_label('TVD')\n",
    "ax.set_xlabel('Bottom Hole Latitude (WGS84)')\n",
    "ax.set_ylabel('Bottom Hole Longitude (WGS84)')\n",
    "ax.set_zlabel('Middle Bakken Producing Depth (TVD)')\n",
    "ax.invert_zaxis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Other palette\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_trisurf(Y,X,Z, cmap=plt.cm.jet, linewidth=0.00)\n",
    "ax.invert_zaxis()\n",
    "ax.set_xlabel('Bottom Hole Latitude (WGS84)')\n",
    "ax.set_ylabel('Bottom Hole Longitude (WGS84)')\n",
    "ax.set_zlabel('Middle Bakken Producing Depth (TVD)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# reformat \n",
    "with open('../housekeeping/verticalWells.pkl', 'rb') as fp:\n",
    "    well_dict = pickle.load(fp)\n",
    "\n",
    "bakkenWells = pd.read_csv('../datasets/envernus/middle bakken wells.CSV')\n",
    "print(bakkenWells)\n",
    "# new_list = []\n",
    "# for k in well_dict:\n",
    "#     new_list.append('../datasets/'+k)\n",
    "\n",
    "# pd.read_csv(new_list[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e011ff45b026eeda34f61f1af40e8d10c36da8bcb30013a241d98cbe0fa4d907"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
