{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of vertical wells, perform TICC (Toeplitz Inverse Covariance Clustering)\n",
    "across the same set of parameters found in each well (GR, Density, Porosity, PE, etc.).\n",
    "\n",
    "First, determine, which wells we have in the vertical portion. Once our raw dataset is derived, \n",
    "attempt to find a proximal set of wells (i.e. where cluster density is maximized), so as to \n",
    "increase the probability of encountering similar geostructure amoungst neighboring wells.\n",
    "\n",
    "Once the refined dataset is derived, run TICC script in parallel so as to save in computation time.\n",
    "Evaluate results and determine if the sensor-to-drillbit data gap can be accurately predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "d_drive_dataset = 'D:\\horizontal drilling\\datasets'\n",
    "\n",
    "# read in maxDepth.txt and look at the data, try to see what logs are most common in vertical leg.\n",
    "\n",
    "def findMostCommonFeature(file):\n",
    "    with open(file, 'r') as f:\n",
    "        d = json.loads(f.read())\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    columnDict = {'Depth':0,\n",
    "                'GR':0\n",
    "                }\n",
    "\n",
    "    for k,v in d.items():\n",
    "        # only look at 1000 or so instances. \n",
    "        if i < min(1000, len(d.keys())):\n",
    "            with open('..\\\\datasets\\\\scrapedWells\\\\'+k, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                try:\n",
    "                    columns = lines[0].strip().split(',')[1:]\n",
    "                    columnDict['Depth'] += 1\n",
    "                    for _c in columns[1:]:\n",
    "                        if _c not in columnDict and _c not in ['#Depth', 'DEPTH', 'DEPT']:\n",
    "                            columnDict[_c] = 1\n",
    "                        else:\n",
    "                            columnDict[_c] += 1 \n",
    "                except Exception as err:\n",
    "                    pass\n",
    "        # df = pd.read_csv(k, index_col=0)\n",
    "        i += 1\n",
    "\n",
    "    sortedColumnDict = dict(sorted(columnDict.items(), \n",
    "                                    key=lambda item: item[1],\n",
    "                                    reverse=True))\n",
    "\n",
    "    print(sortedColumnDict)\n",
    "\n",
    "findMostCommonFeature('../housekeeping/maxDepths.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists.\n",
      "trajectories all accounted for.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "def find_dense_well_clusters(dataset_directory):\n",
    "    # read in excel dataset from file.\n",
    "    \n",
    "    df = pd.read_csv(dataset_directory)\n",
    "\n",
    "    # determine the highest well density / county.\n",
    "    county_well_count = df['County/Parish'].value_counts()\n",
    "    densest_county, c_num_wells = [(k,v) for k,v in county_well_count.items()][0]\n",
    "\n",
    "    # create subset of wells in this county.\n",
    "    county_wells_df = df.loc[df['County/Parish'] == densest_county]\n",
    "\n",
    "    # redo for highest field density.\n",
    "    field_well_count = county_wells_df['Field'].value_counts()\n",
    "    densest_field, f_num_wells    = [(k,v) for k,v in field_well_count.items()][0]\n",
    "        \n",
    "    densest_field_df = county_wells_df.loc[df['Field'] == densest_field]\n",
    "\n",
    "    # densest subplay\n",
    "    subplay_well_count = densest_field_df['DI Subplay'].value_counts()\n",
    "    densest_subplay, sp_num_wells    = [(k,v) for k,v in subplay_well_count.items()][0]\n",
    "    densest_subplay_df = densest_field_df.loc[df['DI Subplay'] == densest_subplay]\n",
    "\n",
    "    densest_subplay_df.to_csv('../datasets/densest_subplay.csv', index=False)\n",
    "    return densest_subplay_df\n",
    "\n",
    "def create_trajectory_directories(subplay_df):\n",
    "\n",
    "    # just rename this for ease of use.\n",
    "    df = subplay_df\n",
    "\n",
    "    api14s = [int(api / 1e4) for api in df['API14'].tolist()]\n",
    "\n",
    "    root_dir = 'D:\\horizontal drilling\\datasets\\scrapedWells'\n",
    "\n",
    "    log_folders = sorted([int(path.split('\\\\')[-1]) for path in glob.glob(f'{root_dir}/*')], reverse=True)\n",
    "\n",
    "    wells_with_data = sorted(list(set(api14s) & set(log_folders)))\n",
    "   \n",
    "    try:\n",
    "        os.mkdir('../datasets/mckenzie-county')\n",
    "    except OSError:\n",
    "        print('directory already exists.')\n",
    "        pass\n",
    "    \n",
    "    for api14 in wells_with_data:\n",
    "        try:\n",
    "            os.mkdir(f'../datasets/mckenzie-county/{api14}')\n",
    "        except OSError:\n",
    "            pass\n",
    "    \n",
    "    return wells_with_data   \n",
    "\n",
    "def verify_folders_have_trajectories(api14s):\n",
    "\n",
    "    root_dir = r\"C:\\Users\\rush\\Desktop\\programming\\horizontal drilling\\datasets\\mckenzie-county\"\n",
    "\n",
    "    empty_dirs = [api14 for api14 in api14s if not os.listdir(root_dir + f'\\{api14}')]\n",
    "\n",
    "    # returns True if no empty dirs, returns the list of empty dirs if exist.\n",
    "\n",
    "    if empty_dirs:\n",
    "        return empty_dirs\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # learned that for some reason, the api14's were not properly saved somewhere along the line. so \n",
    "    # i went BACK to drillinginfo, and did the query again knowing mckenzie county was very dense.\n",
    "\n",
    "    densest_subplay = find_dense_well_clusters(dataset_directory='../datasets/mckenzie county.CSV')\n",
    "\n",
    "    api14s = create_trajectory_directories(densest_subplay)\n",
    "\n",
    "    if verify_folders_have_trajectories(api14s):\n",
    "        print('trajectories all accounted for.')\n",
    "    else:\n",
    "        print(verify_folders_have_trajectories(api14s))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e011ff45b026eeda34f61f1af40e8d10c36da8bcb30013a241d98cbe0fa4d907"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
